import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error


# Example usage
df = pd.read_csv("your_dataset.csv")

X = df.drop(columns=["target"])  # replace 'target' with your target column
y = df["target"]

# Identify feature types
categorical_features = X.select_dtypes(include=['int64', 'int32']).columns.tolist()  # Already label encoded
numerical_features = X.select_dtypes(include=['float64', 'float32']).columns.tolist()



# Create transformers
scaler = StandardScaler()

# Apply scaler to both numerical and categorical (already label-encoded) features
preprocessor = ColumnTransformer(
    transformers=[
        ("num", scaler, numerical_features),
        ("cat", scaler, categorical_features)
    ]
)




# Pipeline
ridge_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Ridge())
])

# Hyperparameter grid
ridge_params = {
    'regressor__alpha': [0.01, 0.1, 1, 10, 100]
    'regressor__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']

}

ridge_grid = GridSearchCV(ridge_pipeline, ridge_params, cv=5, scoring='neg_root_mean_squared_error')
ridge_grid.fit(X, y)

print("Best Ridge RMSE:", -ridge_grid.best_score_)
print("Best Params:", ridge_grid.best_params_)



xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))
])

xgb_params = {
    'regressor__n_estimators': [100, 200, 300],
    'regressor__max_depth': [3, 5, 7],
    'regressor__learning_rate': [0.01, 0.1, 0.2],
    'regressor__subsample': [0.7, 0.8, 1.0]
}

xgb_search = RandomizedSearchCV(xgb_pipeline, xgb_params, n_iter=20, scoring='neg_root_mean_squared_error', cv=3, random_state=42, n_jobs=-1)
xgb_search.fit(X, y)

print("Best XGBoost RMSE:", -xgb_search.best_score_)
print("Best Params:", xgb_search.best_params_)









# Load your dataset
df = pd.read_csv("your_dataset.csv")  # Replace with actual path

# Split features and target
X = df.drop(columns=["target"])  # Replace 'target' with your actual target column
y = df["target"]

# Split the data into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify numerical and categorical features
categorical_features = X.select_dtypes(include=['int64', 'int32']).columns.tolist()
numerical_features = X.select_dtypes(include=['float64', 'float32']).columns.tolist()

# Combine all features for scaling
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[categorical_features + numerical_features] = scaler.fit_transform(X_train[categorical_features + numerical_features])
X_test_scaled[categorical_features + numerical_features] = scaler.transform(X_test[categorical_features + numerical_features])




ridge = Ridge()

ridge_params = {
    'alpha': np.logspace(-3, 3, 20),
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']
}

ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)
ridge_grid.fit(X_train_scaled, y_train)

print("Best Ridge RMSE (CV):", -ridge_grid.best_score_)
print("Best Params:", ridge_grid.best_params_)

# Evaluate on test set
ridge_best = ridge_grid.best_estimator_
y_pred_ridge = ridge_best.predict(X_test_scaled)
print("Test RMSE (Ridge):", mean_squared_error(y_test, y_pred_ridge, squared=False))




xgb = XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

xgb_params = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9, 12],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2, 0.4],
    'reg_alpha': [0, 0.01, 0.1, 1],
    'reg_lambda': [0.1, 1, 10]
}

xgb_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=xgb_params,
    n_iter=50,
    scoring='neg_root_mean_squared_error',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

xgb_search.fit(X_train_scaled, y_train)

print("Best XGBoost RMSE (CV):", -xgb_search.best_score_)
print("Best Params:", xgb_search.best_params_)

# Evaluate on test set
xgb_best = xgb_search.best_estimator_
y_pred_xgb = xgb_best.predict(X_test_scaled)
print("Test RMSE (XGBoost):", mean_squared_error(y_test, y_pred_xgb, squared=False))









| Feature                   | Correlation with `InsurancePremium` | Interpretation                                                                              |
| ------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------- |
| **DeductibleAmount**      | **+0.54**                           | Strong positive relationship. As deductible increases, insurance premium tends to increase. |
| **PreExistingConditions** | **+0.44**                           | Moderate positive correlation. More pre-existing conditions â†’ higher premium.               |
| **Age**                   | +0.16                               | Weak positive relationship. Older individuals pay slightly more on average.                 |
| **BMI**                   | +0.13                               | Slight increase in premium with higher BMI.                                                 |
| **NumberOfDependents**    | +0.14                               | Small effect: more dependents â†’ slightly higher premium.                                    |
| **YearsSinceLastClaim**   | **-0.13**                           | Weak negative correlation. More time since last claim â†’ slightly lower premium.             |
| **AnnualIncome**          | +0.05                               | Almost no impact on premium.                                                                |







ðŸ“Š Inter-feature Correlations

These are mostly very weak or near zero, meaning:

Low multicollinearity â€” good for models like linear regression.
Features are mostly independent.
Examples:

BMI vs AnnualIncome: 0.00 â†’ no relation.
NumberOfDependents vs AnnualIncome: 0.01 â†’ no relation.
Age vs any feature: 0.00 or 0.01 â†’ very little correlation









| Metric                  | **Ridge Regression** | **XGBoost Regression** |
| ----------------------- | -------------------- | ---------------------- |
| **RMSE (CV)**           | 181.06               | **115.10** âœ…           |
| **Test RMSE**           | 182.96               | **112.98** âœ…           |
| **MSE**                 | 33,473.72            | **12,765.17** âœ…        |
| **RÂ² Score (Training)** | 0.9316               | **0.9739** âœ…           |
