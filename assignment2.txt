import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error


# Example usage
df = pd.read_csv("your_dataset.csv")

X = df.drop(columns=["target"])  # replace 'target' with your target column
y = df["target"]

# Identify feature types
categorical_features = X.select_dtypes(include=['int64', 'int32']).columns.tolist()  # Already label encoded
numerical_features = X.select_dtypes(include=['float64', 'float32']).columns.tolist()



# Create transformers
scaler = StandardScaler()

# Apply scaler to both numerical and categorical (already label-encoded) features
preprocessor = ColumnTransformer(
    transformers=[
        ("num", scaler, numerical_features),
        ("cat", scaler, categorical_features)
    ]
)




# Pipeline
ridge_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', Ridge())
])

# Hyperparameter grid
ridge_params = {
    'regressor__alpha': [0.01, 0.1, 1, 10, 100]
    'regressor__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']

}

ridge_grid = GridSearchCV(ridge_pipeline, ridge_params, cv=5, scoring='neg_root_mean_squared_error')
ridge_grid.fit(X, y)

print("Best Ridge RMSE:", -ridge_grid.best_score_)
print("Best Params:", ridge_grid.best_params_)



xgb_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', XGBRegressor(objective='reg:squarederror', random_state=42))
])

xgb_params = {
    'regressor__n_estimators': [100, 200, 300],
    'regressor__max_depth': [3, 5, 7],
    'regressor__learning_rate': [0.01, 0.1, 0.2],
    'regressor__subsample': [0.7, 0.8, 1.0]
}

xgb_search = RandomizedSearchCV(xgb_pipeline, xgb_params, n_iter=20, scoring='neg_root_mean_squared_error', cv=3, random_state=42, n_jobs=-1)
xgb_search.fit(X, y)

print("Best XGBoost RMSE:", -xgb_search.best_score_)
print("Best Params:", xgb_search.best_params_)









# Load your dataset
df = pd.read_csv("your_dataset.csv")  # Replace with actual path

# Split features and target
X = df.drop(columns=["target"])  # Replace 'target' with your actual target column
y = df["target"]

# Split the data into train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Identify numerical and categorical features
categorical_features = X.select_dtypes(include=['int64', 'int32']).columns.tolist()
numerical_features = X.select_dtypes(include=['float64', 'float32']).columns.tolist()

# Combine all features for scaling
scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[categorical_features + numerical_features] = scaler.fit_transform(X_train[categorical_features + numerical_features])
X_test_scaled[categorical_features + numerical_features] = scaler.transform(X_test[categorical_features + numerical_features])




ridge = Ridge()

ridge_params = {
    'alpha': np.logspace(-3, 3, 20),
    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']
}

ridge_grid = GridSearchCV(ridge, ridge_params, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)
ridge_grid.fit(X_train_scaled, y_train)

print("Best Ridge RMSE (CV):", -ridge_grid.best_score_)
print("Best Params:", ridge_grid.best_params_)

# Evaluate on test set
ridge_best = ridge_grid.best_estimator_
y_pred_ridge = ridge_best.predict(X_test_scaled)
print("Test RMSE (Ridge):", mean_squared_error(y_test, y_pred_ridge, squared=False))




xgb = XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

xgb_params = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [3, 5, 7, 9, 12],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.5, 0.7, 0.9, 1.0],
    'min_child_weight': [1, 3, 5, 7],
    'gamma': [0, 0.1, 0.2, 0.4],
    'reg_alpha': [0, 0.01, 0.1, 1],
    'reg_lambda': [0.1, 1, 10]
}

xgb_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=xgb_params,
    n_iter=50,
    scoring='neg_root_mean_squared_error',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

xgb_search.fit(X_train_scaled, y_train)

print("Best XGBoost RMSE (CV):", -xgb_search.best_score_)
print("Best Params:", xgb_search.best_params_)

# Evaluate on test set
xgb_best = xgb_search.best_estimator_
y_pred_xgb = xgb_best.predict(X_test_scaled)
print("Test RMSE (XGBoost):", mean_squared_error(y_test, y_pred_xgb, squared=False))









| Feature                   | Correlation with `InsurancePremium` | Interpretation                                                                              |
| ------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------- |
| **DeductibleAmount**      | **+0.54**                           | Strong positive relationship. As deductible increases, insurance premium tends to increase. |
| **PreExistingConditions** | **+0.44**                           | Moderate positive correlation. More pre-existing conditions ‚Üí higher premium.               |
| **Age**                   | +0.16                               | Weak positive relationship. Older individuals pay slightly more on average.                 |
| **BMI**                   | +0.13                               | Slight increase in premium with higher BMI.                                                 |
| **NumberOfDependents**    | +0.14                               | Small effect: more dependents ‚Üí slightly higher premium.                                    |
| **YearsSinceLastClaim**   | **-0.13**                           | Weak negative correlation. More time since last claim ‚Üí slightly lower premium.             |
| **AnnualIncome**          | +0.05                               | Almost no impact on premium.                                                                |







üìä Inter-feature Correlations

These are mostly very weak or near zero, meaning:

Low multicollinearity ‚Äî good for models like linear regression.
Features are mostly independent.
Examples:

BMI vs AnnualIncome: 0.00 ‚Üí no relation.
NumberOfDependents vs AnnualIncome: 0.01 ‚Üí no relation.
Age vs any feature: 0.00 or 0.01 ‚Üí very little correlation









| Metric                  | **Ridge Regression** | **XGBoost Regression** |
| ----------------------- | -------------------- | ---------------------- |
| **RMSE (CV)**           | 181.06               | **115.10** ‚úÖ           |
| **Test RMSE**           | 182.96               | **112.98** ‚úÖ           |
| **MSE**                 | 33,473.72            | **12,765.17** ‚úÖ        |
| **R¬≤ Score (Training)** | 0.9316               | **0.9739** ‚úÖ           |














Here is your business recommendation based on the Ridge and XGBoost models in a **bullet-style paragraph format**, combining analysis, insights, and actionable suggestions:

---

* The analysis of the dataset using both Ridge and XGBoost regression models shows that certain features have a strong impact on insurance premiums, particularly `DeductibleAmount`, `PreExistingConditions`, `BMI`, and `Age`. These features consistently demonstrated significant correlation or importance in model performance and SHAP analysis, indicating their predictive power in estimating premium values.

* Given that the XGBoost model significantly outperformed Ridge in terms of RMSE (112.98 vs. 182.95) and MSE (12765 vs. 33473), it should be the preferred model for **predicting insurance premiums**. Additionally, its high R¬≤ score (\~0.97) suggests the model explains most of the variance in premium values, making it reliable for business decision-making.

* The company should consider **using this model to personalize premium offers**. A real-time premium calculator could be implemented during customer onboarding, using the XGBoost model to provide quick and accurate premium quotes tailored to individual risk profiles.

* Based on the top influencing features, insurers can create **customer segments** such as low-risk (younger, lower BMI, no chronic illness), medium-risk, and high-risk groups (older age, high BMI, multiple preexisting conditions). This segmentation can help in designing tailored policies and pricing strategies for each segment.

* To improve retention and encourage healthy behavior, the business can offer **incentives such as discounts or cashback** to low-risk customers or to those who actively engage in health improvement programs. This not only rewards healthy customers but also reduces future claim risks.

* For high-risk individuals, instead of charging them excessively, the business can design **value-added plans** that include preventive care, disease management support, or lifestyle counseling, creating a sense of value and care while still managing risk exposure.

* The Ridge model, while less powerful than XGBoost, can be useful in **explainable pricing** due to its linear and interpretable nature. This helps in ensuring **transparency and fairness** in pricing decisions, which is crucial from a regulatory and customer trust standpoint.

* The use of **SHAP values in XGBoost** provides actionable interpretability even for a complex model, allowing underwriters to explain why a certain premium was assigned to a customer, which enhances transparency and supports compliance with insurance regulations.

* The model pipeline can also be used to **streamline and automate underwriting processes**, reducing reliance on manual effort and increasing operational efficiency.

* Lastly, these insights can guide **product development strategies**. For example, launching ‚ÄúPredict & Save‚Äù plans that offer lower premiums if a customer‚Äôs health indicators improve over time can drive engagement and reduce claim costs.

---

Let me know if you'd like this formatted into a **PDF report**, **PowerPoint slide**, or **Word document** for presentation or sharing.
